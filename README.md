# Synthara - AI-Powered Dataset Generation Platform

Synthara is a comprehensive web application that generates structured datasets using AI and real-time web data. The platform combines web scraping, AI processing, and intelligent data structuring to create high-quality datasets for various use cases.

## ðŸš€ Quick Start

For complete setup instructions, see **[COMPLETE_SETUP_GUIDE.md](./COMPLETE_SETUP_GUIDE.md)**

### Prerequisites
- Node.js 18+
- Python 3.8+
- Supabase account
- API keys for AI services

### Installation
```bash
git clone <repository-url>
cd Synthara-1
npm install
```

### Environment Setup
Create `.env.local` with your API keys (see complete guide for details).

### Run the Application
```bash
# Start Crawl4AI service (required for web scraping)
docker run -p 8000:8000 crawl4ai/crawl4ai:latest

# Start the application
npm run dev
```

Visit `http://localhost:3000` to access the application.

## How It Works

### 1. Dataset Generation Flow

**User Input Processing:**
- Users provide natural language prompts describing their data requirements
- The system analyzes prompts to determine optimal data generation strategy
- Users can choose between synthetic data generation or live web data extraction

**Search Query Optimization:**
- AI refines user prompts into optimized Google search queries
- Removes instruction words while preserving location details and specifics
- Users can preview and manually edit refined search queries before execution
- Debounced real-time query refinement (1-second delay) for better UX

**Web Data Collection:**
- Performs intelligent web searches using optimized queries
- Filters search results to prioritize reliable sources (government sites, established organizations)
- Scrapes content from multiple sources simultaneously
- Processes various content types: tables, lists, structured data, plain text

**AI Data Processing:**
- Analyzes scraped content to identify relevant information
- Extracts real data when available from scraped sources
- Generates synthetic data when scraped content is insufficient
- Creates structured datasets with appropriate column schemas and data types

### 2. Real-Time Processing

**Live Progress Tracking:**
- Real-time logging of each processing step
- Progress indicators showing current operation status
- Detailed feedback on data sources and processing decisions
- Error handling with fallback strategies

**Streaming Data Generation:**
- Server-sent events for real-time progress updates
- Non-blocking processing allowing users to monitor generation
- Immediate feedback on search results and content quality

## Tools and Technologies

### Frontend Technologies

**Next.js 14**
- React-based full-stack framework
- App Router for modern routing and layouts
- Server-side rendering and static site generation
- Built-in API routes for backend functionality

**TypeScript**
- Type-safe development environment
- Enhanced code reliability and maintainability
- Improved developer experience with IntelliSense

**Tailwind CSS**
- Utility-first CSS framework
- Responsive design system
- Custom component styling with shadcn/ui

**React Hook Form + Zod**
- Form state management and validation
- Type-safe form schemas
- Real-time validation feedback

**Lucide React**
- Modern icon library
- Consistent iconography throughout the application

### Backend and AI Integration

**OpenRouter API**
- Primary AI service provider
- DeepSeek Chat V3 model integration
- Cost-effective AI processing with high-quality outputs
- Fallback model support for reliability

**Web Scraping Engine**
- Custom scraping implementation
- Multiple search engine support (Google, Bing, DuckDuckGo)
- Content type detection and parsing
- Anti-bot detection handling

**Puppeteer**
- Headless browser automation
- JavaScript-rendered content scraping
- Dynamic page interaction capabilities

### Data Processing

**Zod Schema Validation**
- Runtime type checking and validation
- Data structure enforcement
- Input sanitization and error handling

**CSV Generation**
- Structured data export functionality
- Proper CSV formatting and encoding
- Download management system

### Development and Deployment

**Node.js Runtime**
- Server-side JavaScript execution
- Package management with npm
- Development and production environments

**Environment Configuration**
- Secure API key management
- Environment-specific settings
- Configuration validation

## Key Features

### Search Query Intelligence
- **AI-Powered Refinement:** Converts natural language prompts into effective search queries
- **Location Preservation:** Maintains important geographical and contextual details
- **Manual Override:** Users can edit AI-generated search queries
- **Real-Time Preview:** Immediate feedback on query optimization

### Multi-Source Data Collection
- **Intelligent Source Selection:** Prioritizes reliable and authoritative sources
- **Content Quality Assessment:** Evaluates scraped content before processing
- **Fallback Strategies:** Multiple approaches when primary scraping fails
- **Structured Data Detection:** Identifies tables, lists, and organized content

### AI Data Processing
- **Content Analysis:** Deep understanding of scraped material
- **Real Data Extraction:** Pulls actual information from web sources when available
- **Synthetic Generation:** Creates realistic data when real extraction isn't possible
- **Schema Discovery:** Automatically determines optimal data structure

### User Experience
- **Real-Time Feedback:** Live progress tracking and status updates
- **Transparent Processing:** Clear indication of data sources and generation methods
- **Error Recovery:** Graceful handling of failures with user-friendly messages
- **Export Options:** Easy CSV download with proper formatting

## Data Generation Strategies

### Web Data Extraction
1. **Search Optimization:** Refine user queries for better web search results
2. **Source Filtering:** Select high-quality, reliable web sources
3. **Content Scraping:** Extract text, tables, and structured data
4. **Data Analysis:** AI processes scraped content to identify relevant information
5. **Structure Creation:** Generate appropriate dataset schema and populate with real data

### Synthetic Data Generation
1. **Prompt Analysis:** Understand user requirements and context
2. **Pattern Recognition:** Identify data patterns and relationships
3. **Realistic Generation:** Create believable data that matches user specifications
4. **Quality Assurance:** Ensure data consistency and logical relationships

## Processing Pipeline

**Input Validation â†’ Query Optimization â†’ Web Search â†’ Content Scraping â†’ AI Analysis â†’ Data Structuring â†’ Export Generation**

Each step includes error handling, progress reporting, and quality checks to ensure reliable dataset generation.

## Performance Optimization

- **Concurrent Processing:** Parallel web scraping and data processing
- **Caching Strategies:** Reduced redundant API calls and processing
- **Streaming Responses:** Real-time progress updates without blocking
- **Resource Management:** Efficient memory and CPU usage during generation

## Security and Reliability

- **Input Sanitization:** Secure handling of user inputs and web content
- **Rate Limiting:** Respectful web scraping practices
- **Error Boundaries:** Graceful failure handling and recovery
- **Data Validation:** Comprehensive checking of generated datasets

Synthara combines the power of AI with real-time web data to create a unique dataset generation experience that adapts to user needs while maintaining transparency about data sources and generation methods.
