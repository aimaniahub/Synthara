
'use server';
/**
 * @fileOverview A flow for generating synthetic data by searching the web, scraping content, and structuring it.
 *
 * This flow orchestrates a RAG (Retrieval-Augmented Generation) pipeline:
 * 1. Refine: Uses AI to optimize the user's prompt for better web search results.
 * 2. Search: Uses SerpApi to find relevant web pages using the optimized query.
 * 3. Filter: Uses an AI model to select the most promising URLs from the search results.
 * 4. Scrape: Uses Firecrawl to scrape the content from the filtered URLs in parallel.
 * 5. Structure: Uses an AI model to synthesize the scraped content into structured data (JSON/CSV).
 *
 * - generateFromWeb - The main function that orchestrates the entire pipeline.
 * - GenerateFromWebInput - The input type for the generateFromWeb function.
 * - GenerateFromWebOutput - The return type for the generateFromWeb function.
 */

import { z } from 'zod';
import { getGoogleSearchResults, type SearchResult } from '@/services/serpapi-service';
import { scrapeContent, scrapeContentFallback } from '@/services/firecrawl-service';
import { type GenerateDataOutput, type Column } from './generate-data-flow';
import { FileManagerService, type ScrapedSource } from '@/services/file-manager-service';
import { OpenRouterService } from '@/services/openrouter-service';

// Helper function to create OpenRouter service
function createOpenRouterService(): OpenRouterService | null {
  try {
    const apiKey = process.env.OPENROUTER_API_KEY;
    const baseUrl = process.env.OPENROUTER_BASE_URL;
    const model = process.env.OPENROUTER_MODEL;
    const siteUrl = process.env.OPENROUTER_SITE_URL;
    const siteName = process.env.OPENROUTER_SITE_NAME;

    if (!apiKey) {
      console.warn('[OpenRouter] API key not found in environment variables');
      return null;
    }

    return new OpenRouterService({
      apiKey,
      baseUrl,
      model,
      siteUrl,
      siteName
    });
  } catch (error: any) {
    console.error('[OpenRouter] Failed to create service:', error);
    return null;
  }
}

// Enhanced fallback data extraction for when AI processing fails
function extractSimpleDataFromContent(content: string, userPrompt: string, numRows: number): any {
    const lines = content.split('\n');
    const extractedData: any[] = [];

    console.log(`[Fallback] Analyzing ${lines.length} lines for structured data patterns...`);
    console.log(`[Fallback] User prompt: ${userPrompt}`);
    console.log(`[Fallback] Target rows: ${numRows}`);

    // Try to extract tabular data from markdown tables
    const tableRows = lines.filter(line => line.includes('|') && !line.includes('---'));

    if (tableRows.length > 1) {
        console.log(`[Fallback] Found ${tableRows.length} table rows, attempting to parse...`);

        // Parse table headers
        const headerRow = tableRows[0];
        const headers = headerRow.split('|').map(h => h.trim()).filter(h => h.length > 0);

        if (headers.length > 1) {
            // Parse data rows
            for (let i = 1; i < Math.min(tableRows.length, numRows + 1); i++) {
                const dataRow = tableRows[i];
                const cells = dataRow.split('|').map(c => c.trim()).filter(c => c.length > 0);

                if (cells.length === headers.length) {
                    const rowData: any = {};
                    headers.forEach((header, index) => {
                        rowData[header] = cells[index] || '';
                    });
                    extractedData.push(rowData);
                }
            }
        }
    }

    // If no tables found, try to extract structured text patterns
    if (extractedData.length === 0) {
        console.log(`[Fallback] No tables found, trying structured text extraction...`);

        // Look for key-value patterns (common in product listings, specifications, etc.)
        const keyValueLines = lines.filter(line => {
            const trimmed = line.trim();
            return trimmed.includes(':') && trimmed.length > 10 && trimmed.length < 200;
        });

        if (keyValueLines.length >= numRows) {
            console.log(`[Fallback] Found ${keyValueLines.length} key-value pairs, extracting...`);

            for (let i = 0; i < Math.min(keyValueLines.length, numRows); i++) {
                const line = keyValueLines[i].trim();
                const [key, ...valueParts] = line.split(':');
                const value = valueParts.join(':').trim();

                if (key && value) {
                    extractedData.push({
                        attribute: key.trim(),
                        value: value,
                        source: 'Web scraping'
                    });
                }
            }
        }
    }

    // If still no data, try to extract list-based data
    if (extractedData.length === 0) {
        console.log(`[Fallback] No structured patterns found, trying list-based extraction...`);

        const listItems = lines.filter(line => {
            const trimmed = line.trim();
            return (trimmed.startsWith('- ') || trimmed.startsWith('* ') || /^\d+\.\s/.test(trimmed))
                   && trimmed.length > 10;
        });

        for (let i = 0; i < Math.min(listItems.length, numRows); i++) {
            const item = listItems[i].replace(/^[-*\d.]\s*/, '').trim();

            // Try to extract structured information from the item
            const parts = item.split(/[:\-‚Äì‚Äî]/).map(p => p.trim());

            if (parts.length >= 2) {
                extractedData.push({
                    item: parts[0],
                    description: parts.slice(1).join(' - '),
                    source: 'Web scraping'
                });
            } else {
                extractedData.push({
                    item: item,
                    source: 'Web scraping'
                });
            }
        }
    }

    console.log(`[Fallback] Extracted ${extractedData.length} entries using pattern matching`);

    // If we don't have enough data, try to generate more synthetic entries
    if (extractedData.length < numRows) {
        console.log(`[Fallback] Need ${numRows} rows but only found ${extractedData.length}, generating synthetic data...`);

        // Generate additional synthetic entries based on the user prompt
        const additionalNeeded = numRows - extractedData.length;
        for (let i = 0; i < additionalNeeded; i++) {
            // Create synthetic data based on the prompt keywords
            const promptLower = userPrompt.toLowerCase();
            let syntheticEntry: any = {};

            if (promptLower.includes('plant') || promptLower.includes('disease')) {
                syntheticEntry = {
                    name: `Plant Disease ${i + 1}`,
                    discovery_year: '2025',
                    researcher: `Dr. Researcher ${i + 1}`,
                    location: `Research Lab ${i + 1}`,
                    source: 'Synthetic (based on prompt)'
                };
            } else if (promptLower.includes('movie') || promptLower.includes('review')) {
                syntheticEntry = {
                    title: `Movie Title ${i + 1}`,
                    review: `This is a sample movie review ${i + 1} with detailed analysis.`,
                    rating: Math.floor(Math.random() * 5) + 1,
                    reviewer: `Reviewer ${i + 1}`,
                    source: 'Synthetic (based on prompt)'
                };
            } else if (promptLower.includes('customer') || promptLower.includes('data')) {
                syntheticEntry = {
                    name: `Customer ${i + 1}`,
                    email: `customer${i + 1}@example.com`,
                    age: Math.floor(Math.random() * 50) + 20,
                    source: 'Synthetic (based on prompt)'
                };
            } else {
                // Generic synthetic data
                syntheticEntry = {
                    item: `Item ${i + 1}`,
                    description: `Description for item ${i + 1}`,
                    value: Math.floor(Math.random() * 100) + 1,
                    source: 'Synthetic (based on prompt)'
                };
            }

            extractedData.push(syntheticEntry);
        }

        console.log(`[Fallback] Generated ${additionalNeeded} synthetic entries, total: ${extractedData.length}`);
    }

    // Final fallback: extract meaningful text content if no structured data found
    if (extractedData.length === 0) {
        console.log(`[Fallback] No structured data found, extracting meaningful text content...`);

        // Get lines with substantial content (not headers, navigation, etc.)
        const meaningfulLines = lines.filter(line => {
            const trimmed = line.trim();
            return trimmed.length > 20 &&
                   trimmed.length < 300 &&
                   !trimmed.startsWith('#') &&
                   !trimmed.startsWith('*') &&
                   !trimmed.includes('|') &&
                   !/^[\s\-_=]+$/.test(trimmed) &&
                   !trimmed.toLowerCase().includes('navigation') &&
                   !trimmed.toLowerCase().includes('menu') &&
                   !trimmed.toLowerCase().includes('footer');
        });

        if (meaningfulLines.length > 0) {
            for (let i = 0; i < Math.min(meaningfulLines.length, numRows); i++) {
                const content = meaningfulLines[i].trim();
                extractedData.push({
                    content: content,
                    index: i + 1,
                    source: 'Web scraping',
                    type: 'Text content'
                });
            }
            console.log(`[Fallback] Extracted ${extractedData.length} text content entries`);
        }
    }

    // Return extracted data or minimal result if still nothing found
    if (extractedData.length === 0) {
        console.log(`[Fallback] No extractable content found, creating minimal dataset`);

        // Create at least one row with basic information
        const basicData = [{
            message: 'Data extraction completed',
            status: 'Scraped content available but no structured data detected',
            suggestion: 'Try refining your search query or configure AI models for better extraction',
            timestamp: new Date().toISOString()
        }];

        return {
            generatedJsonString: JSON.stringify(basicData),
            detectedSchema: [
                { name: 'message', type: 'String' },
                { name: 'status', type: 'String' },
                { name: 'suggestion', type: 'String' },
                { name: 'timestamp', type: 'String' }
            ],
            feedback: 'Web scraping successful but no structured data patterns detected. Consider using AI models for better data extraction or refining your search query.'
        };
    }

    return {
        generatedJsonString: JSON.stringify(extractedData.slice(0, numRows)),
        detectedSchema: Object.keys(extractedData[0] || {}).map(key => ({
            name: key,
            type: 'String'
        })),
        feedback: `Extracted ${extractedData.length} rows using fallback pattern matching due to AI quota limits. Data quality may be limited without AI processing.`
    };
}



// Define schemas needed for this flow
const ColumnSchema = z.object({
  name: z.string().describe('The name of the column.'),
  type: z.string().describe('The inferred data type of the column (e.g., String, Integer, Float, Boolean, Date).'),
});

const LLMGenerateDataOutputSchema = z.object({
  generatedJsonString: z.string().describe('A JSON string representing an array of generated data objects (rows). This string MUST be a valid JSON array of objects. For example: [{"name": "Alice", "age": 30}, {"name": "Bob", "age": 25}]'),
  detectedSchema: z.array(ColumnSchema).describe('The schema (column names and types) inferred from the generated data. This should be derived from the structure of generatedJsonString.'),
  feedback: z.string().optional().describe('Any feedback or notes from the generation process, like warnings or suggestions.'),
});

// Helper function to convert JSON to CSV
async function jsonToCsv(jsonData: Array<Record<string, any>>): Promise<string> {
  if (!jsonData || jsonData.length === 0) {
    return "";
  }
  const keys = Object.keys(jsonData[0]);
  const csvRows = [
    keys.join(','), // Header row
    ...jsonData.map(row =>
      keys.map(key => {
        const value = row[key];
        // Escape commas and quotes in CSV values
        if (typeof value === 'string' && (value.includes(',') || value.includes('"') || value.includes('\n'))) {
          return `"${value.replace(/"/g, '""')}"`;
        }
        return value;
      }).join(',')
    )
  ];
  return csvRows.join('\n');
}

const GenerateFromWebInputSchema = z.object({
  prompt: z.string().describe('The natural language prompt describing the data to generate, which will be used as a search query.'),
  numRows: z.number().optional().default(50).describe('The number of data rows to generate. No maximum limit.'),
  logger: z.any().optional().describe('Optional logger for real-time progress updates.'),
  refinedSearchQuery: z.string().optional().describe('Pre-refined search query to use instead of refining the prompt.'),
});
export type GenerateFromWebInput = z.infer<typeof GenerateFromWebInputSchema>;
export type GenerateFromWebOutput = GenerateDataOutput;

export async function generateFromWeb(input: GenerateFromWebInput): Promise<GenerateFromWebOutput> {
  return generateFromWebFlow(input);
}

// Function to refine user query using OpenRouter DeepSeek
async function refineSearchQuery(userPrompt: string): Promise<{ searchQuery: string; reasoning: string }> {
    try {
        const openRouterService = createOpenRouterService();
        if (!openRouterService) {
            throw new Error('OpenRouter service not available');
        }

        const refinementPrompt = `Extract the core search terms from this user request and create an effective Google search query.

RULES:
1. Remove instruction words: "generate", "create", "provide", "I need", "give me", "find", "get", "make", "build", "retrieve"
2. Remove data collection words: "data", "dataset", "table", "list", "information", "details"
3. KEEP important specifics: locations, company names, time periods, specific domains
4. KEEP essential qualifiers: "latest", "recent", "current", specific industries
5. Use 3-8 words that capture the main topic with key specifics
6. Prioritize terms that will find relevant, current content

EXAMPLES:
"I need a table listing common diseases affecting mango and apple trees" ‚Üí "mango apple tree diseases"
"Generate NSE FII and DII net inflow data for June" ‚Üí "NSE FII DII June 2024"
"Create customer data with demographics" ‚Üí "customer demographics analysis"
"Provide job postings in Bangalore for AI/ML roles" ‚Üí "jobs Bangalore AI ML latest"
"Retrieve latest job postings in Bangalore, India with salary details" ‚Üí "latest jobs Bangalore India salary"
"Get latest iPhone models with prices in India" ‚Üí "latest iPhone models prices India"
"Find startup funding information for 2024" ‚Üí "startup funding 2024"
"Generate stock market data for Indian companies" ‚Üí "Indian stock market data"
"Create sales data for retail companies in Mumbai" ‚Üí "retail sales Mumbai companies"

User request: "${userPrompt}"

Respond with a JSON object containing:
{
  "searchQuery": "optimized search query that preserves key specifics",
  "reasoning": "brief explanation of what was kept and removed"
}`;

        const result = await openRouterService.processScrapedContent({
            userPrompt: refinementPrompt,
            numRows: 1,
            scrapedContent: ''
        });

        // Parse the JSON response
        const parsed = JSON.parse(result.jsonString);
        if (parsed && parsed[0] && parsed[0].searchQuery) {
            return {
                searchQuery: parsed[0].searchQuery,
                reasoning: parsed[0].reasoning || 'Query refined using OpenRouter DeepSeek'
            };
        }

        // Fallback if parsing fails
        throw new Error('Invalid response format');

    } catch (error: any) {
        console.error('[Query Refinement] OpenRouter failed:', error.message);
        // Fallback to simple keyword extraction
        return {
            searchQuery: extractSimpleKeywords(userPrompt),
            reasoning: 'Used fallback keyword extraction due to AI service error'
        };
    }
}


// Simple URL filtering function (replaces AI prompt)
function filterBestUrls(query: string, searchResults: any[]): string[] {
    // Simple filtering logic based on URL patterns and titles
    const priorityDomains = [
        'gov', 'edu', 'org', // Official sources
        'bloomberg.com', 'reuters.com', 'yahoo.com', 'google.com', // Financial/News
        'indeed.com', 'linkedin.com', 'glassdoor.com', 'naukri.com', // Jobs
        'amazon.com', 'flipkart.com', 'myntra.com', // E-commerce
        'wikipedia.org', 'investopedia.com' // Reference
    ];

    const filteredResults = searchResults
        .filter(result => {
            const url = result.link || result.url || '';
            const title = result.title || '';

            // Skip social media and forums
            if (url.includes('reddit.com') || url.includes('facebook.com') ||
                url.includes('twitter.com') || url.includes('instagram.com')) {
                return false;
            }

            // Prefer priority domains
            const hasPriorityDomain = priorityDomains.some(domain => url.includes(domain));

            // Check if title/URL seems relevant to query
            const queryWords = query.toLowerCase().split(' ');
            const relevantWords = queryWords.filter(word =>
                title.toLowerCase().includes(word) || url.toLowerCase().includes(word)
            );

            return hasPriorityDomain || relevantWords.length >= 2;
        })
        .slice(0, 5) // Take top 5
        .map(result => result.link || result.url);

    return filteredResults;
}

// Note: Old AI model constants removed - now using OpenRouter DeepSeek

// Note: Old prompt cache and createStructurePrompt function removed - now using OpenRouter DeepSeek

// Helper function to clean AI response from markdown code blocks
function cleanAIResponse(response: string): string {
    if (typeof response !== 'string') {
        return response;
    }

    // Remove markdown code blocks
    let cleaned = response.replace(/```json\s*/g, '').replace(/```\s*/g, '');

    // Try to find JSON content between braces
    const jsonMatch = cleaned.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
        cleaned = jsonMatch[0];
    }

    return cleaned.trim();
}

// Simple keyword extraction fallback (no AI needed)
function extractSimpleKeywords(userPrompt: string): string {
    // Remove common instruction words but keep important specifics
    const instructionWords = [
        'generate', 'create', 'provide', 'give me', 'i need', 'find', 'get', 'make', 'build', 'retrieve',
        'data', 'dataset', 'table', 'list', 'information', 'details', 'for', 'with', 'of', 'the',
        'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'from', 'by', 'as', 'is', 'are'
    ];

    // Important words to prioritize (locations, qualifiers, domains)
    const importantWords = [
        'bangalore', 'mumbai', 'delhi', 'chennai', 'hyderabad', 'pune', 'kolkata', 'india',
        'latest', 'recent', 'current', 'new', 'jobs', 'job', 'posting', 'postings',
        'salary', 'price', 'cost', 'stock', 'market', 'nse', 'bse', 'company', 'companies'
    ];

    // Split into words and filter
    const words = userPrompt.toLowerCase()
        .replace(/[^\w\s]/g, ' ')
        .split(/\s+/)
        .filter(word => word.length > 2 && !instructionWords.includes(word));

    // Prioritize important words first, then take others
    const priorityWords = words.filter(word => importantWords.includes(word));
    const otherWords = words.filter(word => !importantWords.includes(word));

    // Combine priority words with other words, limit to 6 words max
    const finalWords = [...priorityWords, ...otherWords].slice(0, 6);

    return finalWords.join(' ');
}

// Helper function to generate simple fallback search queries
function generateFallbackQueries(optimizedQuery: string, originalPrompt: string): string[] {
    const fallbackQueries: string[] = [];

    // Extract only the most important keywords
    const keyTerms = originalPrompt.toLowerCase()
        .replace(/[^\w\s]/g, ' ')
        .split(/\s+/)
        .filter(term => term.length > 2 &&
            !['the', 'and', 'for', 'with', 'data', 'generate', 'create', 'provide', 'latest', 'get', 'find'].includes(term));

    // Strategy 1: Use just the main 2-3 keywords
    if (keyTerms.length >= 2) {
        fallbackQueries.push(keyTerms.slice(0, 3).join(' '));
    }

    // Strategy 2: Remove time words for broader search
    const broaderQuery = optimizedQuery
        .replace(/\b(june|july|august|2024|2025|latest|recent|current|today)\b/gi, '')
        .replace(/\s+/g, ' ')
        .trim();
    if (broaderQuery && broaderQuery !== optimizedQuery && broaderQuery.length > 3) {
        fallbackQueries.push(broaderQuery);
    }

    // Strategy 3: Simple domain-specific fallbacks
    const prompt = originalPrompt.toLowerCase();

    if (prompt.includes('nse') || prompt.includes('stock')) {
        fallbackQueries.push('NSE stock');
        fallbackQueries.push('Indian stock market');
    }

    if (prompt.includes('fii') || prompt.includes('dii')) {
        fallbackQueries.push('FII DII');
        fallbackQueries.push('foreign investors India');
    }

    if (prompt.includes('job') || prompt.includes('employment')) {
        fallbackQueries.push('jobs India');
    }

    if (prompt.includes('price') || prompt.includes('cost')) {
        fallbackQueries.push('prices India');
    }

    // Remove duplicates and very short queries
    return [...new Set(fallbackQueries)].filter(q => q && q.trim().length > 5);
}

// Helper function to process content with OpenRouter DeepSeek
async function tryAIModelsWithFallback(
    userPrompt: string,
    numRows: number,
    scrapedContent: string,
    logger: any
): Promise<any> {
    // Use OpenRouter DeepSeek as the primary and only AI model
    try {
        logger.log(`ü§ñ Processing with OpenRouter DeepSeek Chat V3 (${scrapedContent.length} chars)...`);

        const openRouterService = createOpenRouterService();
        if (!openRouterService) {
            throw new Error('OpenRouter service not available. Please check your OPENROUTER_API_KEY environment variable.');
        }

        const result = await openRouterService.processScrapedContent({
            userPrompt,
            numRows,
            scrapedContent
        });

        // Convert OpenRouter response to expected format
        console.log(`[WebFlow] OpenRouter result:`, JSON.stringify(result, null, 2));

        let parsedData;
        try {
            parsedData = JSON.parse(result.jsonString);
            console.log(`[WebFlow] Parsed data from OpenRouter:`, parsedData);
        } catch (parseError) {
            console.error(`[WebFlow] Failed to parse OpenRouter jsonString:`, parseError);
            console.error(`[WebFlow] Raw jsonString:`, result.jsonString);
            throw new Error(`OpenRouter returned invalid JSON: ${parseError.message}`);
        }

        if (!Array.isArray(parsedData) || parsedData.length === 0) {
            console.error(`[WebFlow] OpenRouter returned invalid data structure:`, parsedData);
            throw new Error('OpenRouter did not return a valid array of data');
        }

        const convertedResult = {
            output: {
                jsonString: result.jsonString,
                detectedSchema: result.detectedSchema,
                feedback: result.feedback
            }
        };

        console.log(`[WebFlow] Converted result for return:`, JSON.stringify(convertedResult, null, 2));
        logger.success(`‚úÖ OpenRouter DeepSeek succeeded! Generated ${parsedData.length} rows with ${result.detectedSchema.length} columns`);
        return convertedResult;

    } catch (error: any) {
        logger.error(`‚ùå OpenRouter processing failed: ${error.message}`);
        console.error(`[WebFlow] OpenRouter error:`, error);

        // Check for common OpenRouter issues
        if (error.message?.includes('API key') || error.message?.includes('authentication')) {
            logger.error(`üîë OpenRouter API key issue. Please check your OPENROUTER_API_KEY environment variable.`);
        } else if (error.message?.includes('rate limit') || error.message?.includes('quota')) {
            logger.error(`üö¶ OpenRouter rate limit reached. Please wait or upgrade your plan.`);
        } else if (error.message?.includes('model') || error.message?.includes('deepseek')) {
            logger.error(`ü§ñ DeepSeek model issue. The model might be temporarily unavailable.`);
        }

        // Throw the error to trigger fallback data extraction
        throw new Error(`OpenRouter DeepSeek processing failed: ${error.message}`);
    }
}

// Old AI prompt removed - now using OpenRouter DeepSeek

// Request deduplication cache to prevent multiple concurrent identical requests
const activeRequests = new Map<string, Promise<any>>();

async function generateFromWebFlow(input: GenerateFromWebInput): Promise<GenerateFromWebOutput> {
    // Request deduplication to prevent multiple concurrent identical requests
    // Add timestamp to force new request for testing fixes
    const requestKey = `${input.prompt.trim()}-${input.numRows || 10}-${Date.now()}`;

    const startTime = Date.now();
    let feedbackLog = "Starting live web data generation process...\n";
    const logger = input.logger;

    // Temporarily disable cache for testing
    // Check if this exact request is already in progress
    // if (activeRequests.has(requestKey)) {
    //     console.log(`[WebFlow] üîÑ Duplicate request detected, waiting for existing request: ${requestKey.substring(0, 50)}...`);
    //     try {
    //         return await activeRequests.get(requestKey);
    //     } catch (error) {
    //         // If the existing request failed, remove it and continue with new request
    //         activeRequests.delete(requestKey);
    //         console.log(`[WebFlow] ‚ö†Ô∏è Previous request failed, proceeding with new request`);
    //     }
    // }

    console.log(`[WebFlow] Starting web generation with prompt: "${input.prompt.substring(0, 100)}..."`);
    console.log(`[WebFlow] Target rows: ${input.numRows}`);
    console.log(`[WebFlow] AI Model: OpenRouter DeepSeek Chat V3 (Free)`);
    console.log(`[WebFlow] Process started at: ${new Date().toISOString()}`);

    // Store this request in cache to prevent duplicates
    const processingPromise = processRequest();
    activeRequests.set(requestKey, processingPromise);

    // Ensure cleanup happens regardless of success/failure
    processingPromise.finally(() => {
        activeRequests.delete(requestKey);
    });

    return await processingPromise;

    async function processRequest() {
        // Helper functions for logging
    const log = (message: string) => {
      feedbackLog += `${message}\n`;
      logger?.log(message);
    };

    const logSuccess = (message: string) => {
      feedbackLog += `${message}\n`;
      logger?.success(message);
    };

    const logError = (message: string) => {
      feedbackLog += `${message}\n`;
      logger?.error(message);
    };

    const logProgress = (step: string, current: number, total: number, details?: string) => {
      logger?.progress(step, current, total, details);
    };

    log("üöÄ Starting web-based data generation...");
    logProgress('Initialization', 1, 7, 'Setting up web scraping pipeline');

    // 1. Use pre-refined search query or refine the search query for better web search results
    let optimizedQuery: string = input.prompt; // Default fallback

    if (input.refinedSearchQuery && input.refinedSearchQuery.trim()) {
        // Use the pre-refined search query from the UI
        optimizedQuery = input.refinedSearchQuery.trim();
        logSuccess(`‚úÖ Using pre-refined search query: "${optimizedQuery}"`);
        feedbackLog += `Pre-refined search query used: "${optimizedQuery}"\n`;
        logProgress('Query Ready', 2, 7, 'Using refined search query from UI');
    } else {
        // Refine the query using AI
        try {
            logProgress('Query Optimization', 2, 7, 'AI is refining your search query for better results');
            log(`üîç Refining search query from: "${input.prompt}"...`);

            const queryRefinement = await refineSearchQuery(input.prompt);

            // Validate the response
            if (queryRefinement && queryRefinement.searchQuery && queryRefinement.searchQuery.trim()) {
                optimizedQuery = queryRefinement.searchQuery.trim();
                logSuccess(`‚úÖ Query optimized by OpenRouter DeepSeek: "${optimizedQuery}"`);
                feedbackLog += `OpenRouter optimized search query: "${optimizedQuery}"\n`;
                feedbackLog += `Reasoning: ${queryRefinement.reasoning || 'No reasoning provided'}\n`;
            } else {
                throw new Error("Query refinement returned empty or invalid result");
            }
        } catch (error: any) {
            logError(`‚ùå OpenRouter query refinement failed: ${error.message}. Using simple keyword extraction.`);

            // Fallback to simple keyword extraction
            optimizedQuery = extractSimpleKeywords(input.prompt);

            if (optimizedQuery && optimizedQuery.trim()) {
                logSuccess(`‚úÖ Query optimized by keyword extraction: "${optimizedQuery}"`);
                feedbackLog += `Simple keyword extraction used: "${optimizedQuery}"\n`;
            } else {
                logError(`‚ùå Keyword extraction also failed. Using original prompt.`);
                optimizedQuery = input.prompt;
                feedbackLog += `Both AI and keyword extraction failed, using original prompt: ${error.message}\n`;
            }
        }
    }

    // Final validation to ensure we have a query
    if (!optimizedQuery || !optimizedQuery.trim()) {
        return { generatedRows: [], detectedSchema: [], feedback: `Error: No valid search query available. Original prompt: "${input.prompt}"` };
    }

    // 2. Search the web with optimized query (with fallback strategies)
    let searchResults: SearchResult[];
    try {
        logProgress('Web Search', 3, 7, 'Searching the web for relevant sources');
        log(`üåê Searching Google for: "${optimizedQuery}"...`);

        searchResults = await getGoogleSearchResults(optimizedQuery);

        // If no results, try fallback search strategies
        if (searchResults.length === 0) {
            log(`‚ö†Ô∏è No results for optimized query, trying fallback searches...`);

            // Fallback 1: Try broader search terms
            const fallbackQueries = generateFallbackQueries(optimizedQuery, input.prompt);

            for (const fallbackQuery of fallbackQueries) {
                try {
                    log(`üîÑ Trying fallback query: "${fallbackQuery}"`);
                    searchResults = await getGoogleSearchResults(fallbackQuery);
                    if (searchResults.length > 0) {
                        logSuccess(`‚úÖ Found ${searchResults.length} results with fallback query`);
                        feedbackLog += `Used fallback search query: "${fallbackQuery}"\n`;
                        break;
                    }
                } catch (fallbackError) {
                    log(`‚ö†Ô∏è Fallback query failed: ${fallbackError}`);
                }
            }
        }

        if (searchResults.length === 0) {
            throw new Error("No relevant search results found after trying multiple search strategies.");
        }

        logSuccess(`‚úÖ Found ${searchResults.length} potential sources`);
        feedbackLog += `Found ${searchResults.length} potential sources.\n`;
    } catch (error: any) {
        logError(`‚ùå Web search failed: ${error.message}`);
        return { generatedRows: [], detectedSchema: [], feedback: `Failed during web search: ${error.message}. Try using more general search terms or check if the data exists online.` };
    }

    // 3. Filter links with AI (with fallback to manual selection)
    let filteredUrls: string[];
    try {
        logProgress('Link Filtering', 4, 7, 'AI is selecting the best sources for your query');
        log("ü§ñ AI is filtering the best sources...");

        filteredUrls = filterBestUrls(input.prompt, searchResults);
        if (filteredUrls.length === 0) {
            throw new Error("Could not select any suitable URLs from search results.");
        }

        logSuccess(`‚úÖ AI selected ${filteredUrls.length} high-quality URLs to scrape`);
        feedbackLog += `AI selected ${filteredUrls.length} URLs to scrape.\n`;
    } catch (error: any) {
        logError(`‚ùå AI link filtering failed: ${error.message}. Using top search results.`);
        feedbackLog += `AI link filtering failed, using top search results: ${error.message}\n`;

        // Fallback: Use top 5 search results
        filteredUrls = searchResults.slice(0, Math.min(5, searchResults.length)).map(result => result.link);
        logSuccess(`‚úÖ Selected top ${filteredUrls.length} search results as fallback`);
        feedbackLog += `Selected top ${filteredUrls.length} search results as fallback.\n`;
    }

    // 4. Scrape content and create structured file
    let structuredFilePath: string;
    try {
        logProgress('Web Scraping', 5, 7, 'Extracting content from selected websites');
        log(`üï∑Ô∏è Scraping content from ${filteredUrls.length} selected URLs...`);

        // Helper function to check if a Firecrawl error is retryable with fallback scraping
        const isRetryableFirecrawlError = (error: string): boolean => {
            const retryableErrors = [
                'Bad gateway',
                'Gateway timeout',
                'Service unavailable',
                'Connection timeout',
                'Network error',
                'server connection issue',
                '502',
                '503',
                '504'
            ];

            return retryableErrors.some(retryableError =>
                error.toLowerCase().includes(retryableError.toLowerCase())
            );
        };

        // Optimize parallel scraping with better error handling and timeout
        const SCRAPE_TIMEOUT = 35000; // 35 seconds per URL (increased for better success rate)
        const MIN_CONTENT_SIZE = 100; // Minimum content size to be considered valid
        const MIN_SUCCESS_RATE = 0.2; // At least 20% of URLs must succeed (reduced for better tolerance)

        const scrapePromises = filteredUrls.map(async (url) => {
            const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error('Scraping timeout')), SCRAPE_TIMEOUT)
            );

            try {
                // Try main scraping first
                const result = await Promise.race([scrapeContent(url), timeoutPromise]);

                // If main scraping failed with retryable error, try fallback
                if (result.error && isRetryableFirecrawlError(result.error)) {
                    console.log(`[WebFlow] Main scraping failed for ${url}, trying fallback: ${result.error}`);
                    const fallbackResult = await scrapeContentFallback(url);

                    if (!fallbackResult.error) {
                        console.log(`[WebFlow] Fallback scraping successful for ${url}`);
                        return fallbackResult;
                    }

                    console.log(`[WebFlow] Both main and fallback scraping failed for ${url}`);
                }

                return result;
            } catch (error) {
                return { error: error instanceof Error ? error.message : 'Unknown error', urlScraped: url };
            }
        });

        const scrapeResults = await Promise.allSettled(scrapePromises);

        // Collect successful scrapes into structured format
        const scrapedSources: ScrapedSource[] = [];
        const failedUrls: string[] = [];
        let totalContentLength = 0;

        scrapeResults.forEach((result, index) => {
            const url = filteredUrls[index];

            if (result.status === 'fulfilled' && result.value.markdown) {
                const contentLength = result.value.markdown.length;

                // Skip sources with very little content (likely errors or empty pages)
                if (contentLength < MIN_CONTENT_SIZE) {
                    failedUrls.push(url);
                    log(`  ‚ö†Ô∏è Skipped ${url} - insufficient content (${contentLength} chars)`);
                    feedbackLog += `  - Skipped ${url}: insufficient content\n`;
                    return;
                }

                // Use full content from each source - no size limits
                const markdown = result.value.markdown;

                scrapedSources.push({
                    url,
                    markdown,
                    contentType: 'Web Content',
                    timestamp: new Date()
                });
                totalContentLength += markdown.length;
                log(`  ‚úÖ Successfully scraped: ${url} (${contentLength.toLocaleString()} chars)`);
                feedbackLog += `  - Successfully scraped: ${url} (${contentLength} chars)\n`;
            } else {
                const errorMsg = result.status === 'rejected'
                    ? result.reason?.message || 'Unknown error'
                    : result.value?.error || 'No content returned';
                failedUrls.push(url);

                // Use different icons for different types of errors
                const isTimeout = errorMsg.includes('timeout') || errorMsg.includes('timed out');
                const isRateLimit = errorMsg.includes('rate limit') || errorMsg.includes('Rate limit');

                if (isTimeout) {
                    log(`  ‚è±Ô∏è Timeout: ${url} (website took too long to respond)`);
                    feedbackLog += `  - Timeout: ${url} (website took too long)\n`;
                } else if (isRateLimit) {
                    log(`  üö¶ Rate limited: ${url} (too many requests, will retry later)`);
                    feedbackLog += `  - Rate limited: ${url} (temporary limit)\n`;
                } else {
                    log(`  ‚ùå Failed to scrape ${url}: ${errorMsg}`);
                    feedbackLog += `  - Failed to scrape ${url}: ${errorMsg}\n`;
                }
            }
        });

        // Log summary of scraping results
        if (failedUrls.length > 0) {
            log(`‚ö†Ô∏è ${failedUrls.length} URLs failed to scrape`);
            feedbackLog += `${failedUrls.length} URLs failed to scrape.\n`;
        }

        // Calculate success rate
        const successRate = scrapedSources.length / filteredUrls.length;

        if (scrapedSources.length === 0) {
            logError("‚ùå Failed to scrape any content from the selected URLs");

            // Log detailed failure information
            log(`üìä Scraping summary: ${failedUrls.length} failed URLs:`);
            failedUrls.slice(0, 5).forEach((url, index) => {
                log(`   ${index + 1}. ${url}`);
            });
            if (failedUrls.length > 5) {
                log(`   ... and ${failedUrls.length - 5} more URLs failed`);
            }

            // Provide helpful error message with suggestions
            const errorMsg = `Failed to scrape content from all ${filteredUrls.length} selected URLs. ` +
                           `Common causes: network timeouts, server errors (500/503), or access restrictions. ` +
                           `Try with a different search query or check your internet connection.`;

            throw new Error(errorMsg);
        }

        // Check if we have enough successful scrapes
        if (successRate < MIN_SUCCESS_RATE) {
            log(`‚ö†Ô∏è Low success rate: ${Math.round(successRate * 100)}% (${scrapedSources.length}/${filteredUrls.length})`);
            log(`‚ö†Ô∏è This may result in limited data quality. Consider trying different search terms.`);
        }

        // Log success with details about partial failures if any
        if (failedUrls.length > 0) {
            logSuccess(`‚úÖ Successfully scraped ${scrapedSources.length}/${filteredUrls.length} sources (${Math.round(successRate * 100)}% success rate)`);
            log(`‚ö†Ô∏è Failed URLs: ${failedUrls.slice(0, 3).join(', ')}${failedUrls.length > 3 ? ` and ${failedUrls.length - 3} more` : ''}`);
        } else {
            logSuccess(`‚úÖ Successfully scraped all ${scrapedSources.length} sources (100% success rate)`);
        }

        // Create structured markdown file
        logProgress('File Processing', 6, 7, 'Creating structured files and organizing content');
        log(`üìÅ Creating structured file from ${scrapedSources.length} sources...`);

        structuredFilePath = await FileManagerService.createStructuredMarkdownFile(
            input.prompt,
            scrapedSources
        );

        logSuccess(`‚úÖ Created structured file with ${totalContentLength.toLocaleString()} characters`);
        feedbackLog += `Created structured file: ${structuredFilePath}\n`;
        feedbackLog += `Total content: ${totalContentLength.toLocaleString()} characters across ${scrapedSources.length} sources\n`;

    } catch (error: any) {
        return { generatedRows: [], detectedSchema: [], feedback: `Failed during web scraping: ${error.message}` };
    }

    // 5. Structure the data using the final AI prompt with structured file
    try {
        logProgress('AI Processing', 7, 7, 'AI is extracting and structuring the data');
        log("üß† AI is analyzing structured file and extracting data...");

        // Read the structured file content
        const structuredContent = await FileManagerService.readStructuredFile(structuredFilePath);
        log(`üìñ Processing structured file: ${structuredContent.length.toLocaleString()} characters`);

        // Send scraped content to frontend for transparency
        if (logger && typeof logger.info === 'function') {
            try {
                // Send a special message type for scraped content
                logger.info(`SCRAPED_CONTENT:${structuredContent}`);
            } catch (error) {
                console.warn('Failed to send scraped content to frontend:', error);
            }
        }
        feedbackLog += `Structured file content: ${structuredContent.length} characters\n`;

        const effectiveNumRows = input.numRows || 50;
        log(`üéØ Requesting ${effectiveNumRows} rows of structured data`);
        feedbackLog += `Requesting ${effectiveNumRows} rows of data from structured sources\n`;

        console.log(`[WebFlow] üß† OPENROUTER DEEPSEEK CONTENT ANALYSIS ACTIVATED`);
        console.log(`[WebFlow] Calling AI with structured file: "${structuredFilePath}"`);
        console.log(`[WebFlow] File size: ${structuredContent.length} characters, requesting ${effectiveNumRows} rows`);
        console.log(`[WebFlow] Primary: OpenRouter DeepSeek Chat V3 (Free)`);

        // Handle large content by chunking if necessary
        const MAX_CONTENT_SIZE = 80000; // Reduced from unlimited to prevent model overload
        let processedContent = structuredContent;

        if (structuredContent.length > MAX_CONTENT_SIZE) {
            log(`‚ö†Ô∏è Large content detected (${structuredContent.length} chars), chunking for better processing...`);
            // Take the first portion and last portion to maintain context
            const firstPart = structuredContent.substring(0, MAX_CONTENT_SIZE * 0.6);
            const lastPart = structuredContent.substring(structuredContent.length - MAX_CONTENT_SIZE * 0.4);
            processedContent = firstPart + "\n\n[... content truncated for processing ...]\n\n" + lastPart;
            feedbackLog += `Large content chunked: ${structuredContent.length} ‚Üí ${processedContent.length} characters\n`;
        }

        log(`üìÑ Processing content: ${processedContent.length} characters`);
        feedbackLog += `Processing content: ${processedContent.length} characters for comprehensive data extraction\n`;

        let llmOutput;
        let aiProcessingSucceeded = false;

        try {
            // Use OpenRouter DeepSeek for all AI processing
            log("üîÑ Processing large content with OpenRouter DeepSeek Chat V3 (optimized for large context)...");
            const result = await tryAIModelsWithFallback(
                input.prompt,
                effectiveNumRows,
                processedContent,
                { log, success: logSuccess, error: logError }
            );
            // Handle OpenRouter response format
            llmOutput = result.output || result;
            aiProcessingSucceeded = true;
            logSuccess(`‚úÖ AI processing completed successfully`);
            console.log(`[WebFlow] AI call successful, processing response...`);

            // CRITICAL DEBUG: Log the exact structure we received
            console.log(`[WebFlow] CRITICAL DEBUG - result structure:`, JSON.stringify(result, null, 2));
            console.log(`[WebFlow] CRITICAL DEBUG - result.output structure:`, JSON.stringify(result.output, null, 2));
            console.log(`[WebFlow] CRITICAL DEBUG - llmOutput structure:`, JSON.stringify(llmOutput, null, 2));
        } catch (aiError: any) {
            console.error(`[WebFlow] OpenRouter DeepSeek failed:`, aiError);
            logError(`‚ùå OpenRouter processing failed: ${aiError.message}`);
            log("üîÑ Falling back to pattern matching extraction...");
            // Don't throw here - let it fall through to fallback extraction
        }

        // Initialize variables for extracted data
        let jsonString: string | undefined;
        let detectedSchema: any[] | undefined;
        let feedback: string | undefined;

        // Check if AI processing succeeded and we have valid output
        if (aiProcessingSucceeded && llmOutput) {
            console.log(`[WebFlow] AI response received: Success`);
            console.log(`[WebFlow] llmOutput structure:`, JSON.stringify(llmOutput, null, 2));

            // Handle case where response is a string (markdown wrapped JSON)
            if (typeof llmOutput === 'string') {
                try {
                    const cleanedResponse = cleanAIResponse(llmOutput);
                    const parsedResponse = JSON.parse(cleanedResponse);
                    llmOutput = parsedResponse;
                    console.log(`[WebFlow] Cleaned and parsed string response:`, JSON.stringify(llmOutput, null, 2));
                } catch (parseError) {
                    console.error(`[WebFlow] Failed to parse string response:`, parseError);
                    console.error(`[WebFlow] Raw response:`, llmOutput);
                }
            }

            // Extract response data from OpenRouter format
            jsonString = llmOutput?.jsonString || llmOutput?.output?.jsonString || llmOutput?.generatedJsonString;
            detectedSchema = llmOutput?.detectedSchema || llmOutput?.output?.detectedSchema;
            feedback = llmOutput?.feedback || llmOutput?.output?.feedback;

            console.log(`[WebFlow] Extracted values:`, {
                jsonString: jsonString?.substring(0, 100) + '...',
                jsonStringLength: jsonString?.length,
                detectedSchemaLength: detectedSchema?.length,
                feedbackLength: feedback?.length
            });

            console.log(`[WebFlow] AI response details:`, {
                hasOutput: !!llmOutput,
                hasJsonString: !!jsonString,
                jsonStringLength: jsonString?.length || 0,
                hasSchema: !!detectedSchema,
                schemaLength: detectedSchema?.length || 0,
                feedback: feedback?.substring(0, 100) || 'No feedback'
            });

            log("üîç AI response received, processing results...");
            feedbackLog += "AI response received, processing...\n";

            if (!jsonString) {
                logError(`‚ùå AI failed to generate JSON. Available keys: ${Object.keys(llmOutput).join(', ')}`);
                feedbackLog += `AI output keys: ${Object.keys(llmOutput).join(', ')}\n`;
                log("üîÑ Falling back to pattern matching due to invalid AI output...");
                aiProcessingSucceeded = false; // Force fallback
            }
        }

        // If AI processing failed or produced invalid output, use fallback extraction
        if (!aiProcessingSucceeded || !jsonString) {
            console.log(`[WebFlow] Using fallback extraction due to AI failure`);
            console.log(`[WebFlow] Debug - aiProcessingSucceeded: ${aiProcessingSucceeded}, jsonString defined: ${!!jsonString}`);
            log("üîÑ AI processing unavailable, using pattern matching extraction...");
            feedbackLog += "Using fallback pattern matching extraction due to AI processing failure.\n";

            // Jump to fallback extraction (this will be handled in the catch block)
            throw new Error("AI processing failed, using fallback extraction");
        }

        // Additional safety check
        if (!jsonString) {
            console.error(`[WebFlow] CRITICAL: jsonString is undefined even though checks passed!`);
            console.error(`[WebFlow] aiProcessingSucceeded: ${aiProcessingSucceeded}`);
            console.error(`[WebFlow] llmOutput:`, llmOutput);
            throw new Error("Critical error: jsonString is undefined");
        }

        log(`üìù Processing JSON string (${jsonString.length} characters)`);
        feedbackLog += `Generated JSON string length: ${jsonString.length}\n`;

        let parsedRows;
        try {
            parsedRows = JSON.parse(jsonString);
            log(`‚úÖ Successfully parsed ${Array.isArray(parsedRows) ? parsedRows.length : 'non-array'} rows`);
            feedbackLog += `Successfully parsed ${Array.isArray(parsedRows) ? parsedRows.length : 'non-array'} rows\n`;
        } catch (parseError: any) {
            logError(`‚ùå JSON parse error: ${parseError.message}`);
            log(`Raw JSON preview: ${jsonString.substring(0, 200)}...`);
            feedbackLog += `JSON parse error: ${parseError.message}\n`;
            feedbackLog += `Raw JSON string: ${jsonString.substring(0, 500)}...\n`;
            throw new Error(`Failed to parse generated JSON: ${parseError.message}`);
        }

        if (!Array.isArray(parsedRows) || parsedRows.length === 0) {
            logError(`‚ùå Invalid data format: expected non-empty array, got ${typeof parsedRows} with ${Array.isArray(parsedRows) ? parsedRows.length : 'N/A'} items`);

            // Provide more helpful feedback based on the content
            let helpfulMessage = "AI was unable to extract structured data from the scraped content.";
            if (processedContent.length < 100) {
                helpfulMessage += " The scraped content was too short or didn't contain relevant data.";
            } else if (!processedContent.toLowerCase().includes(input.prompt.toLowerCase().split(' ')[0])) {
                helpfulMessage += " The scraped content may not be relevant to your query.";
            } else {
                helpfulMessage += " The content may not contain the specific data you're looking for in a structured format.";
            }

            helpfulMessage += " Try refining your search terms or looking for different data sources.";

            return {
                generatedRows: [],
                generatedCsv: '',
                detectedSchema: [],
                feedback: `${feedbackLog}\n\n${helpfulMessage}\n\nScraped content preview:\n${processedContent.substring(0, 500)}...`
            };
        }

        const generatedCsv = await jsonToCsv(parsedRows);

        // Final success logging
        logSuccess(`üéâ Successfully generated ${parsedRows.length} rows with ${detectedSchema?.length || 0} columns!`);
        log(`üìä CSV generated with ${generatedCsv.split('\n').length - 1} rows`);

        feedbackLog += `Generated CSV with ${generatedCsv.split('\n').length - 1} rows\n`;
        feedbackLog += feedback || "\nData structuring complete.";

        const finalResult = {
            generatedRows: parsedRows,
            generatedCsv,
            detectedSchema: detectedSchema || [],
            feedback: feedbackLog,
        };

        console.log(`[WebFlow] Final result: ${parsedRows.length} rows, CSV length: ${generatedCsv.length}, schema: ${detectedSchema?.length || 0} columns`);

        // Clean up old temporary files to manage disk space
        try {
            await FileManagerService.cleanupOldFiles(2); // Clean files older than 2 hours
            log(`üßπ Cleaned up old temporary files`);
        } catch (cleanupError) {
            // Don't fail the main process if cleanup fails
            console.warn(`[WebFlow] Cleanup warning: ${cleanupError}`);
        }

        return finalResult;

    } catch (error: any) {
        // Always try fallback extraction if AI processing fails and we have scraped content
        const isQuotaError = error.message.includes('429') || error.message.includes('quota') || error.message.includes('Too Many Requests');
        const isApiError = error.message.includes('API') || error.message.includes('401') || error.message.includes('403') || error.message.includes('500');
        const isAiFailure = error.message.includes('AI generated invalid data') || error.message.includes('Failed to parse generated JSON') || error.message.includes('AI processing failed');

        // Try fallback for any AI-related error if we have scraped content
        if (structuredFilePath && (isQuotaError || isApiError || isAiFailure || error.message.includes('AI'))) {
            if (isQuotaError) {
                logError(`‚ö†Ô∏è AI quota exceeded. Attempting fallback data extraction...`);
                feedbackLog += `AI quota exceeded during data structuring. Attempting fallback extraction.\n`;
            } else if (isApiError) {
                logError(`‚ö†Ô∏è AI API error detected. Attempting fallback data extraction...`);
                feedbackLog += `AI API error during data structuring. Attempting fallback extraction.\n`;
            } else {
                logError(`‚ö†Ô∏è AI processing failed. Attempting fallback data extraction...`);
                feedbackLog += `AI processing failed during data structuring. Attempting fallback extraction.\n`;
            }

            try {
                // Read the structured content and try simple extraction
                const structuredContent = await FileManagerService.readStructuredFile(structuredFilePath);
                const effectiveNumRows = input.numRows || 50;

                log(`üîÑ Using fallback pattern matching to extract ${effectiveNumRows} rows...`);
                const fallbackResult = extractSimpleDataFromContent(structuredContent, input.prompt, effectiveNumRows);

                if (fallbackResult.generatedJsonString) {
                    const parsedRows = JSON.parse(fallbackResult.generatedJsonString);
                    const generatedCsv = await jsonToCsv(parsedRows);

                    logSuccess(`‚úÖ Fallback extraction successful: ${parsedRows.length} rows generated`);
                    feedbackLog += `Fallback extraction successful: ${parsedRows.length} rows\n`;
                    feedbackLog += fallbackResult.feedback + '\n';
                    feedbackLog += `Note: For better results, ensure OpenRouter API key is properly configured.\n`;

                    return {
                        generatedRows: parsedRows,
                        generatedCsv,
                        detectedSchema: fallbackResult.detectedSchema || [],
                        feedback: feedbackLog,
                    };
                }
            } catch (fallbackError: any) {
                logError(`‚ùå Fallback extraction also failed: ${fallbackError.message}`);
                feedbackLog += `Fallback extraction failed: ${fallbackError.message}\n`;
            }

            feedbackLog += `AI quota exceeded during data structuring. Please try again later or configure alternative AI models.\n`;
            feedbackLog += `Available models: OpenRouter DeepSeek Chat V3\n`;
            feedbackLog += `Current model: deepseek/deepseek-chat-v3-0324:free\n`;
            feedbackLog += `Scraped content is available in: ${structuredFilePath}\n`;
        } else {
            feedbackLog += `Error during final data structuring: ${error.message}\n`;
            feedbackLog += `Stack trace: ${error.stack}\n`;
        }

        console.log(`[WebFlow] Error in data structuring: ${error.message}`);
        console.log(`[WebFlow] Feedback log: ${feedbackLog}`);

        // Final safety net: if we have scraped content but all AI processing failed, try one more fallback
        if (structuredFilePath) {
            try {
                console.log(`[WebFlow] Final safety net: attempting emergency fallback extraction`);
                const structuredContent = await FileManagerService.readStructuredFile(structuredFilePath);
                const emergencyResult = extractSimpleDataFromContent(structuredContent, input.prompt, input.numRows || 10);

                if (emergencyResult.generatedJsonString && JSON.parse(emergencyResult.generatedJsonString).length > 0) {
                    const parsedRows = JSON.parse(emergencyResult.generatedJsonString);
                    const generatedCsv = await jsonToCsv(parsedRows);

                    console.log(`[WebFlow] Emergency fallback successful: ${parsedRows.length} rows`);
                    feedbackLog += `\nEmergency fallback extraction successful: ${parsedRows.length} rows generated.\n`;
                    feedbackLog += `Note: This is basic pattern matching. Configure AI models for better results.\n`;

                    return {
                        generatedRows: parsedRows,
                        generatedCsv,
                        detectedSchema: emergencyResult.detectedSchema || [],
                        feedback: feedbackLog,
                    };
                }
            } catch (emergencyError) {
                console.error(`[WebFlow] Emergency fallback also failed:`, emergencyError);
                feedbackLog += `Emergency fallback also failed: ${emergencyError.message}\n`;
            }
        }

        return { generatedRows: [], detectedSchema: [], feedback: feedbackLog };
    }
    } // End of processRequest function
}

// Clean up old cache entries periodically (every 5 minutes)
setInterval(() => {
    if (activeRequests.size > 0) {
        console.log(`[WebFlow] üßπ Cleaning up ${activeRequests.size} cached requests`);
        activeRequests.clear();
    }
}, 5 * 60 * 1000);
